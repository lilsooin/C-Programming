### char type

char
```C
char ch_a = 'a'; 
char ch_b = ch_a + 1; /* b */
char ch_c = 99;       /* c */
```
- At least 8bit integer type
- Standard is greater than 8bit
- So complier can define char size is 1million size

### ASCII Code
- 'A' = 65
- 'a' = 65 + 32 = 95

### finding char size
- **<limits.h>** header file includes,
		see  **CHAR_BIT**. you can find bit size
- c standard doesn't force exact data size
  - each compile has different data size
- 1 Byte is **CHAR_BIT** 
  - 1 Byte can be 8bit
  - 1 Byte can be 16bit
- Small device needs small data size


### char and ASCII Code
char
```C
char ch_a = 'a'; 
char ch_b = ch_a + 1; /* b */
char ch_c = 99;       /* c */
```
- Integer type
- char is enough to define ASCII code
- ASCII is `0-126`(7bit)(extended ASCII is greater)
  
### char and signed/unsigned
- char is integer type so it has **signed** and **unsigned**
- If signed/unsigned type isn't showed?
- People might think it would be signed type(becauseother integer type is like that)
- You know.. C standard didn't make the rule
- It depends on a compiler
	- singed type or unsigned type
	- Clang Windows is if the type is skipped it is sgined type
